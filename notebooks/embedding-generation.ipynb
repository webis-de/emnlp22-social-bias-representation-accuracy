{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Embedding generation\n",
    "\n",
    "This notebook contains the code to train and generate the different embedding models that are evaluated in our paper.\n",
    "\n",
    "Paper reference: _Splieth√∂ver, Keiff, Wachsmuth (2022): \"No Word Embedding Model Is Perfect: Evaluating the Representation Accuracy for Social Bias in the Media\", EMNLP 2022, Abu Dhabi._\n",
    "\n",
    "Code & Data reference: https://github.com/webis-de/EMNLP-22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data preparation and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the following two cells for any of the embedding models. They load the most common packages and set commonly used variables. They are necessary to run the training cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "import sys\n",
    "\n",
    "from os import listdir, path\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from tokenizers import Tokenizer, normalizers\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import Lowercase, NFD, StripAccents, Strip, Replace\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "PARENT_DIR = path.abspath(\"../src\")\n",
    "sys.path.append(PARENT_DIR)\n",
    "from embedding_bias.config import (\n",
    "    CRAWL_FIRST_YEAR, CRAWL_LAST_YEAR, SENTENCE_ENDING_TOKEN, NEWS_ARTICLE_DB_NAME)\n",
    "from embedding_bias.preprocessing import preprocess_text\n",
    "from embedding_bias.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = path.join(PARENT_DIR.parent, \"data\")\n",
    "DB_PATH = path.join(DATA_DIR, \"raw\", NEWS_ARTICLE_DB_NAME)\n",
    "ALLSIDES_RANKING_PATH = path.join(DATA_DIR, \"raw\", \"allsides-ranking.csv\")\n",
    "OUTLET_CONFIG_PATH = path.join(DATA_DIR, \"raw\", \"outlet-config.json\")\n",
    "WORD_SETS_PATH = path.join(DATA_DIR, \"raw\", \"word-sets.json\")\n",
    "\n",
    "# Target sqlite database\n",
    "target_db_connection = sqlite3.connect(DB_PATH)\n",
    "\n",
    "# Outlet config file\n",
    "outlet_config = pd.read_json(OUTLET_CONFIG_PATH)\n",
    "outlet_selection = outlet_config\n",
    "\n",
    "# Word sets\n",
    "with open(WORD_SETS_PATH, \"r\") as f:\n",
    "    word_sets = json.load(f)\n",
    "\n",
    "# Allsides ranking\n",
    "allsides_ranking = pd.read_csv(ALLSIDES_RANKING_PATH)\n",
    "\n",
    "# Map from outlet name to political orientation\n",
    "outlet_orientation_map = {\n",
    "    o[\"name\"].lower(): o[\"allsides_rating\"]\n",
    "    for i,o in outlet_selection.iterrows()}\n",
    "\n",
    "# Groups of political orientations\n",
    "orientation_groups = {\n",
    "    \"left\": [\"Lean Left\", \"Left\"],\n",
    "    \"center\": [\"Center\"],\n",
    "    \"right\": [\"Lean Right\", \"Right\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## word2vec models (Static embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "W2V_ARTICLE_PREPROCESS_CACHE = path.join(\n",
    "    DATA_DIR, \"processed\", \"nato\", \"articles-preproc-cache.pkl\")\n",
    "W2V_MODEL_PATH = path.join(DATA_DIR, \"models\", \"nato-w2v\")\n",
    "LOW_COUNT_TOKEN_REPLACE = \"<unk>\"\n",
    "\n",
    "# Whitespace tokenization\n",
    "# lowercasing\n",
    "def hf_tokenize(texts):\n",
    "    normalizers_seq = [\n",
    "        Replace(SENTENCE_ENDING_TOKEN, \" \"),\n",
    "        NFD(),\n",
    "        Lowercase(),\n",
    "        StripAccents(),\n",
    "        Strip(),\n",
    "    ]\n",
    "    normalizer = normalizers.Sequence(normalizers_seq)\n",
    "    pre_tokenizer = Whitespace()\n",
    "    tokenizer = Tokenizer(WordLevel(unk_token=LOW_COUNT_TOKEN_REPLACE))\n",
    "    tokenizer.normalizer = normalizer\n",
    "    tokenizer.pre_tokenizer = pre_tokenizer\n",
    "    trainer = WordLevelTrainer(special_tokens=[LOW_COUNT_TOKEN_REPLACE])\n",
    "\n",
    "    print(\"Starting training.\")\n",
    "    tokenizer.train_from_iterator(texts, trainer)\n",
    "\n",
    "    print(\"Starting tokenization.\")\n",
    "    output = [tokenizer.encode_batch(doc) for doc in tqdm(texts, mininterval=5)]\n",
    "\n",
    "    print(\"Extracting tokens.\")\n",
    "    return [[t.tokens for t in doc] for doc in tqdm(output, mininterval=5)]\n",
    "\n",
    "if path.exists(W2V_ARTICLE_PREPROCESS_CACHE):\n",
    "    print(\"Found article cache. Loading from file.\")\n",
    "    with open(W2V_ARTICLE_PREPROCESS_CACHE, \"rb\") as f:\n",
    "        articles = pickle.load(f)\n",
    "else:\n",
    "    print(\"No cache found. Loading from database.\")\n",
    "    articles = get_articles_as_df(\n",
    "        allsides_ranking=allsides_ranking,\n",
    "        db_connection=target_db_connection,\n",
    "        outlet_selection=outlet_selection,\n",
    "        preprocessed=True)\n",
    "    print(\"Sentence-splitting articles.\")\n",
    "    articles.text = articles.text.apply(\n",
    "        lambda x: x.split(SENTENCE_ENDING_TOKEN))\n",
    "    print(\"Tokenizing articles.\")\n",
    "    # Requires min. 128GB mem., but is fast af\n",
    "    articles[\"text_prep\"] = hf_tokenize(articles.text.tolist())\n",
    "    with open(W2V_ARTICLE_PREPROCESS_CACHE, \"wb\") as f:\n",
    "        pickle.dump(articles, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TextCorpora:\n",
    "    \"\"\"Memory-friendly data loader\"\"\"\n",
    "    def __init__(self, documents):\n",
    "        self.docs = documents\n",
    "\n",
    "    def __iter__(self):\n",
    "        for doc in self.docs:\n",
    "            for sent in doc:\n",
    "                yield sent.split()\n",
    "\n",
    "def get_outlet_orientation(outlet: str) -> str:\n",
    "    return outlet_orientation_map[outlet.lower()]\n",
    "\n",
    "def train_embeddings(document_iterator):\n",
    "    return Word2Vec(\n",
    "        document_iterator,\n",
    "        vector_size=300,  # dimensionality of word vectors\n",
    "        window=5,  # max dist. between current and predicted word within a sent.\n",
    "        min_count=5,  # ignores all words with total frequency lower than this\n",
    "        workers=32,  # number of threads to train the model\n",
    "        sg=1, # whether to use skip-gram (1) or cbow (0)\n",
    "        epochs=5 # epochs to train for\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Train word embedding models per orientation\n",
    "for orientation, grouping in orientation_groups.items():\n",
    "    # Train word2vec from data\n",
    "    orientation_articles = articles[\n",
    "        articles.orientation.isin(grouping)].text_prep\n",
    "    print(f\"Flattening articles for {orientation}.\")\n",
    "    articles_flat = [\n",
    "        sentence for doc in orientation_articles for sentence in doc]\n",
    "    print(f\"Training word embedding model '{orientation}' with Word2Vec...\")\n",
    "    orientation_iter = TextCorpora(articles_flat)\n",
    "\n",
    "    word_embedding = train_embeddings(orientation_iter)\n",
    "    word_embedding.save(f\"{W2V_MODEL_PATH}/{orientation}.model\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Frequency agnostic embeddings (FreqAgn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from tokenizers import Tokenizer, normalizers\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import Lowercase, NFD, StripAccents, Strip, Replace\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text_ = text.replace(SENTENCE_ENDING_TOKEN, \" \")\n",
    "    # Moses tokenization, normalization, lowercasing\n",
    "    return word_tokenize(text_.lower())\n",
    "\n",
    "def hf_tokenize(texts):\n",
    "    # IMPROTANT NOTE: REQUIRES tokenizers version >= 12.x\n",
    "    normalizers_seq = [\n",
    "        Replace(SENTENCE_ENDING_TOKEN, \" \"),\n",
    "        NFD(),\n",
    "        Lowercase(),\n",
    "        StripAccents(),\n",
    "        Strip()\n",
    "    ]\n",
    "    normalizer = normalizers.Sequence(normalizers_seq)\n",
    "    pre_tokenizer = Whitespace()\n",
    "    tokenizer = Tokenizer(WordLevel(unk_token=LOW_COUNT_TOKEN_REPLACE))\n",
    "    tokenizer.normalizer = normalizer\n",
    "    tokenizer.pre_tokenizer = pre_tokenizer\n",
    "    trainer = WordLevelTrainer(\n",
    "        vocab_size=MAX_VOCAB_SIZE,\n",
    "        special_tokens=[LOW_COUNT_TOKEN_REPLACE, SENTENCE_ENDING_TOKEN],\n",
    "        show_progress=True)\n",
    "\n",
    "    print(\"Starting training.\")\n",
    "    tokenizer.train_from_iterator([\" \".join(text) for text in texts], trainer)\n",
    "\n",
    "    print(\"Starting tokenization.\")\n",
    "    output = tokenizer.encode_batch([\" \".join(doc) for doc in texts])\n",
    "\n",
    "    print(\"Extracting tokens.\")\n",
    "    return [doc.tokens for doc in tqdm(output, mininterval=5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Loading the original data\n",
    "AWSLSTM_CORPUS_PATH = path.join(DATA_DIR, \"processed\", \"corpus-awd-lstm-format\")\n",
    "ARTICLE_PREPROCESS_CACHE = path.join(\n",
    "    AWSLSTM_CORPUS_PATH, \"articles_preprocessed.pkl\")\n",
    "\n",
    "LOW_COUNT_TOKEN_REPLACE = \"<unk>\"\n",
    "MAX_VOCAB_SIZE = 40000\n",
    "\n",
    "if path.exists(ARTICLE_PREPROCESS_CACHE):\n",
    "    print(\"Found article cache. Loading from file.\")\n",
    "    with open(ARTICLE_PREPROCESS_CACHE, \"rb\") as f:\n",
    "        articles = pickle.load(f)\n",
    "else:\n",
    "    print(\"No cache found. Loading from database.\")\n",
    "    articles = get_articles_as_df(\n",
    "        db_connection=target_db_connection,\n",
    "        outlet_selection=outlet_selection,\n",
    "        allsides_ranking=allsides_ranking)\n",
    "    print(\"Preprocessing articles.\")\n",
    "    articles.text = articles.text.apply(\n",
    "        lambda x: x.split(SENTENCE_ENDING_TOKEN))\n",
    "    # Requires min. 128GB mem., but is fast af\n",
    "    articles[\"text_prep\"] = hf_tokenize(articles.text)\n",
    "    with open(ARTICLE_PREPROCESS_CACHE, \"wb\") as f:\n",
    "        pickle.dump(articles, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LOW_COUNT_THRESHOLD = 5\n",
    "\n",
    "def get_token_counts(articles, threshold=LOW_COUNT_THRESHOLD) -> list:\n",
    "    token_counter = Counter()\n",
    "    for i, row in articles.iterrows():\n",
    "        token_counter.update(row.text_prep)\n",
    "\n",
    "    print(f\"Original dict size: {len(token_counter.keys())}\")\n",
    "    low_counts = [t for t, c in dict(token_counter).items() if c <= threshold]\n",
    "    print(f\"Tokens that don't pass the threshold: {len(low_counts)}\")\n",
    "\n",
    "    return dict(token_counter)\n",
    "\n",
    "# Prepare our data into AWS-LSTM specific format (only needs to be run the first\n",
    "# time).\n",
    "# This is simple white-space separated tokens in a text file\n",
    "# for orientation in articles.orientation.unique():\n",
    "for orientation, grouping in orientation_groups.items():\n",
    "    print(f\"{'=' * 30} {orientation}\")\n",
    "    orientation_filename_safe = orientation.lower().replace(\" \", \"-\")\n",
    "\n",
    "    orientation_articles = articles[articles.orientation.isin(grouping)]\n",
    "    one_percent_articles = (len(orientation_articles) - 1) / 100\n",
    "\n",
    "    # Retrieve tokens with very few mentions to reduce vocab size\n",
    "    print(\"Counting token occurrences.\")\n",
    "    token_counter = get_token_counts(\n",
    "        orientation_articles, threshold=LOW_COUNT_THRESHOLD)\n",
    "    placeholder = f\" {LOW_COUNT_TOKEN_REPLACE} \"\n",
    "\n",
    "    train_indices = [0, int(one_percent_articles * 70)]\n",
    "    test_indices = [\n",
    "        train_indices[1] + 1,\n",
    "        train_indices[1] + 1 + int(one_percent_articles * 20)]\n",
    "    val_indices = [test_indices[1] + 1, -1]\n",
    "\n",
    "    train_file_name = path.join(\n",
    "        AWSLSTM_CORPUS_PATH, orientation_filename_safe, \"train.txt\")\n",
    "    test_file_name = path.join(\n",
    "        AWSLSTM_CORPUS_PATH, orientation_filename_safe, \"test.txt\")\n",
    "    val_file_name = path.join(\n",
    "        AWSLSTM_CORPUS_PATH, orientation_filename_safe, \"valid.txt\")\n",
    "\n",
    "    with open(train_file_name, mode=\"w\") as f:\n",
    "        print(\"Generating train data.\")\n",
    "        for i, row in orientation_articles.iloc[train_indices[0]:train_indices[1]].iterrows():\n",
    "            text_clean = [\n",
    "                t if token_counter[t] > LOW_COUNT_THRESHOLD\n",
    "                    else LOW_COUNT_TOKEN_REPLACE\n",
    "                for t in row.text_prep]\n",
    "            text_joint = \" \".join(text_clean)\n",
    "            f.write(text_joint)\n",
    "            f.write(\"\\n\")\n",
    "    with open(test_file_name, mode=\"w\") as f:\n",
    "        print(\"Generating test data.\")\n",
    "        for i, row in orientation_articles.iloc[test_indices[0]:test_indices[1]].iterrows():\n",
    "            text_clean = [\n",
    "                t if token_counter[t] > LOW_COUNT_THRESHOLD\n",
    "                    else LOW_COUNT_TOKEN_REPLACE \n",
    "                for t in row.text_prep]\n",
    "            text_joint = \" \".join(text_clean)\n",
    "            f.write(text_joint)\n",
    "            f.write(\"\\n\")\n",
    "    with open(val_file_name, mode=\"w\") as f:\n",
    "        print(\"Generating validation data.\")\n",
    "        for i, row in orientation_articles.iloc[val_indices[0]:val_indices[1]].iterrows():\n",
    "            text_clean = [\n",
    "                t if token_counter[t] > LOW_COUNT_THRESHOLD\n",
    "                    else LOW_COUNT_TOKEN_REPLACE\n",
    "                for t in row.text_prep]\n",
    "            text_joint = \" \".join(text_clean)\n",
    "            f.write(text_joint)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps above actually only prepare the data for the embedding generation. To run the training, please use the script `src/Frequency-Agnostic/frage-lstm-train.sh`. Refer to the README file for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Decontextualized Embeddings (Decontext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import torch\n",
    "\n",
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "from flair.data import Sentence\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download some NLTK dependencies\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DECON_CACHE_PATH = path.join(DATA_DIR, \"processed\", \"contextualized2static\")\n",
    "DECON_MODEL_PATH = path.join(DATA_DIR, \"models\", \"contextualized2static\")\n",
    "SIM_EVAL_DATA_DIR = path.join(\n",
    "    PARENT_DIR, \"embedding_evaluation\", \"embedding_evaluation\", \"data\")\n",
    "\n",
    "# Max number of sentences before cutting of (due to CUDA-OOM)\n",
    "MAX_SENTENCE_NUMBER = 1000000\n",
    "\n",
    "# Pre-trained contextualized embedding model\n",
    "embedding_model = TransformerWordEmbeddings(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading news article data\n",
    "articles = get_articles_as_df(\n",
    "    db_connection=target_db_connection,\n",
    "    outlet_selection=outlet_selection,\n",
    "    allsides_ranking=allsides_ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocabulary = get_test_vocabulary(word_sets=word_sets, similarity_eval_data_path=SIM_EVAL_DATA_DIR)\n",
    "vocab_list = list(vocabulary)\n",
    "\n",
    "# Split all articles by sentence; use the cached version, if available\n",
    "sentences_per_orientation_cache_file = path.join(\n",
    "    DECON_CACHE_PATH, \"sentences-tokenized-per-orientation.pkl\")\n",
    "if path.exists(sentences_per_orientation_cache_file):\n",
    "    print(\"Found sentence per orientation cache. Loading from file.\")\n",
    "    with open(sentences_per_orientation_cache_file, \"rb\") as f:\n",
    "        sentences_by_orientation = pickle.load(f)\n",
    "else:\n",
    "    articles.text = articles.text.apply(\n",
    "        lambda x: x.split(SENTENCE_ENDING_TOKEN))\n",
    "    sentences_by_orientation = {}\n",
    "\n",
    "# For each article, collect all sentences and sort them according to the\n",
    "# political orientation of its media outlet.\n",
    "# Use a cache, if available.\n",
    "for orientation, grouping in orientation_groups.items():\n",
    "    print(\"=\" * 30, orientation)\n",
    "\n",
    "    # Collect all sentences of the current orientation; we don't care about\n",
    "    # document levels or specific outlets anymore at this point, so we can have\n",
    "    # them simply in a flat list\n",
    "    orientation_articles = articles[articles.orientation.isin(grouping)]\n",
    "    sentences_tokenized_cache_file = path.join(\n",
    "        DECON_CACHE_PATH, f\"{orientation}-sentences-tokenized.pkl\")\n",
    "\n",
    "    if path.exists(sentences_tokenized_cache_file):\n",
    "        print(\"Found tokenized cache. Loading from file.\")\n",
    "        with open(sentences_tokenized_cache_file, \"rb\") as f:\n",
    "            orientation_sentences_tokenized = pickle.load(f)\n",
    "    else:\n",
    "        print(\"No tokenized cache found. Tokenizing sentences now.\")\n",
    "        orientation_sentences_tokenized = [\n",
    "            word_tokenize(s.lower())\n",
    "            for sents in tqdm(orientation_articles[\"text\"])\n",
    "            for s in sents]\n",
    "        print(\"Caching tokenized sentences.\")\n",
    "        with open(sentences_tokenized_cache_file, \"wb\") as f:\n",
    "            pickle.dump(orientation_sentences_tokenized, f)\n",
    "\n",
    "    # Retrieve sentences of each token\n",
    "    print(\"Retrieving sentences.\")\n",
    "    if orientation in sentences_by_orientation.keys():\n",
    "        print(\"Found orientation in cache.\")\n",
    "        orientation_sentences = sentences_by_orientation[orientation]\n",
    "    else:\n",
    "        orientation_sentences = {}\n",
    "\n",
    "    for word in tqdm(vocab_list):\n",
    "        if word.lower() in orientation_sentences.keys():\n",
    "            continue\n",
    "        orientation_sentences[word.lower()] = [\n",
    "            \" \".join(s)\n",
    "            for s in orientation_sentences_tokenized\n",
    "            if word.lower() in s]\n",
    "\n",
    "    sentences_by_orientation[orientation] = orientation_sentences\n",
    "\n",
    "# Cache result to disk\n",
    "with open(sentences_per_orientation_cache_file, \"wb\") as f:\n",
    "    pickle.dump(sentences_by_orientation, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve contextualized embeddings per token for each sentence\n",
    "def embed_token(token, sentence: str) -> list:\n",
    "    flair_sentence = Sentence(sentence)\n",
    "    embedding_model.embed(flair_sentence)\n",
    "\n",
    "    # Find token position in sentence\n",
    "    # We use the flair sentence here instead of the original string to ensure,\n",
    "    # that the index of the string and the embedding is really the same.\n",
    "    i = flair_sentence.to_original_text().split().index(token)\n",
    "\n",
    "    return flair_sentence[i].embedding\n",
    "\n",
    "# Retrieve all sentences a token appears in and generate pooled embeddings\n",
    "for orientation, grouping in orientation_groups.items():\n",
    "    print(\"=\" * 30, orientation)\n",
    "    orientation_context_embedded_cache_file = path.join(\n",
    "        DECON_CACHE_PATH, f\"orientation-context-embedded-{orientation}.pkl\")\n",
    "\n",
    "    if path.exists(orientation_context_embedded_cache_file):\n",
    "        print(\"Found existing embedding cache. Loading from file.\")\n",
    "        with open(orientation_context_embedded_cache_file, \"rb\") as f:\n",
    "            orientation_context_embeddings = pickle.load(f)\n",
    "    else:\n",
    "        print(\"No embedding cache found.\")\n",
    "        orientation_context_embeddings = {}\n",
    "\n",
    "    # Retrieve the contextualized embeddings per token for each orientation\n",
    "    for token, sentences in sentences_by_orientation[orientation].items():\n",
    "        if len(sentences) < 1:\n",
    "            print(f\"No sentences for token '{token}'. Skipping\")\n",
    "            continue\n",
    "        elif len(sentences) > MAX_SENTENCE_NUMBER:\n",
    "            print(\n",
    "                f\"Found {len(sentences)} sentences for '{token}'. \\\n",
    "                Truncating to {MAX_SENTENCE_NUMBER}.\")\n",
    "            sentences = sentences[:MAX_SENTENCE_NUMBER]\n",
    "        else:\n",
    "            print(f\"Found {len(sentences)} sentences for '{token}'. No truncation necessary.\")\n",
    "\n",
    "        if token in orientation_context_embeddings.keys():\n",
    "            print(f\"Found '{token}' in cache. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(\"Generating token embeddings.\")\n",
    "        token_embeds_lst = []\n",
    "        for s in tqdm(sentences, mininterval=5, leave=False):\n",
    "            token_embeds_lst.append(embed_token(token, s))\n",
    "        token_sentence_embeddings = torch.vstack(token_embeds_lst)\n",
    "\n",
    "        orientation_context_embeddings[token] = {\n",
    "            \"mean_pooled_embedding\": torch.mean(token_sentence_embeddings, dim=0, keepdim=True),\n",
    "            \"max_pooled_embedding\": torch.amax(token_sentence_embeddings, dim=0, keepdim=True),\n",
    "            \"min_pooled_embedding\": torch.amin(token_sentence_embeddings, dim=0, keepdim=True)\n",
    "        }\n",
    "\n",
    "        # Write embeddings for current token to a cache file, as this operation can take some\n",
    "        # time and the process might be interrupted multiple times.\n",
    "        with open(orientation_context_embedded_cache_file, \"wb\") as f:\n",
    "            pickle.dump(orientation_context_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Generating final token->embedding dictionary from the cached embedding file.\n",
    "# Uses the specific pooled embeddings setting defined below.\n",
    "\n",
    "vocabulary = get_test_vocabulary(word_sets=word_sets, similarity_eval_data_path=SIM_EVAL_DATA_DIR)\n",
    "vocab_list = list(vocabulary)\n",
    "pooled_embedding_type = \"mean_pooled_embedding\"\n",
    "\n",
    "# For each orientation, load the embedding cache, copy tensors to CPU and build the final embedding\n",
    "# dictionary. Write the results directly to disk.\n",
    "for orientation, grouping in orientation_groups.items():\n",
    "    print(\"=\" * 30, orientation)\n",
    "    model_path = f\"{DECON_MODEL_PATH}/{orientation}.model\"\n",
    "    orientation_context_embedded_cache_file = path.join(\n",
    "        DECON_CACHE_PATH, f\"orientation-context-embedded-{orientation}.pkl\")\n",
    "\n",
    "    print(\"Loading cached orientation embeddings.\")\n",
    "    orientation_context_embeddings = {}\n",
    "    with open(orientation_context_embedded_cache_file, \"rb\") as f:\n",
    "        orientation_context_embeddings = pickle.load(f)\n",
    "\n",
    "    vocab_size = len(orientation_context_embeddings)\n",
    "    # Retrieve a sample token to get the embedding length\n",
    "    embedding_size = len(orientation_context_embeddings[\"woman\"][pooled_embedding_type][0])\n",
    "\n",
    "    print(\"Copying vectors to CPU and writing to disk.\")\n",
    "    with open(f\"{model_path}\", \"w\") as f:\n",
    "        f.write(f\"{vocab_size} {embedding_size}\\n\")\n",
    "        for token in tqdm(vocabulary, mininterval=2):\n",
    "            try:\n",
    "                token_vector = (\n",
    "                    orientation_context_embeddings[token][pooled_embedding_type][0].tolist())\n",
    "                token_vector_str = \" \".join([str(d) for d in token_vector])\n",
    "                f.write(f\"{token} {token_vector_str}\\n\")\n",
    "            except KeyError:\n",
    "                print(f\"Token {token} not found. Skipping.\")\n",
    "\n",
    "print(\"Generation done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Temporal models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "from flair.data import Sentence\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECON_CACHE_PATH = path.join(DATA_DIR, \"processed\", \"contextualized2static\", \"temporal\")\n",
    "DECON_MODEL_PATH = path.join(DATA_DIR, \"models\", \"contextualized2static\", \"temporal\")\n",
    "SIM_EVAL_DATA_DIR = path.join(\n",
    "    PARENT_DIR, \"embedding_evaluation\", \"embedding_evaluation\", \"data\")\n",
    "\n",
    "# Max number of sentences before cutting of (due to CUDA-OOM)\n",
    "MAX_SENTENCE_NUMBER = 1000000\n",
    "\n",
    "# Pre-trained contextualized embedding model\n",
    "embedding_model = TransformerWordEmbeddings(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading news article data\n",
    "articles = get_articles_as_df(\n",
    "    db_connection=target_db_connection,\n",
    "    outlet_selection=outlet_selection,\n",
    "    allsides_ranking=allsides_ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter articles based on their publication year, so we only keep the timeframe of interest\n",
    "articles = articles[~(articles.date.str.len() < 10)]\n",
    "articles[\"date_dt\"] = pd.to_datetime(articles.date, format=\"%Y-%m-%d\")\n",
    "articles = articles[(articles.date_dt.dt.year >= 2010) & (articles.date_dt.dt.year <= 2021)]\n",
    "articles[\"year\"] = articles.date_dt.apply(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and cache all sentences for each orientation and year\n",
    "\n",
    "vocabulary = get_test_vocabulary(word_sets=word_sets, similarity_eval_data_path=SIM_EVAL_DATA_DIR)\n",
    "vocab_list = list(vocabulary)\n",
    "\n",
    "# Split all articles by sentence; use the cached version, if available\n",
    "sentences_per_orientation_cache_file = path.join(\n",
    "    DECON_CACHE_PATH, \"sentences-tokenized-per-orientation.pkl\"\n",
    "if path.exists(sentences_per_orientation_cache_file):\n",
    "    print(\"Found sentence per orientation cache. Loading from file.\")\n",
    "    with open(sentences_per_orientation_cache_file, \"rb\") as f:\n",
    "        sentences_by_orientation = pickle.load(f)\n",
    "else:\n",
    "    articles.text = articles.text.apply(lambda x: x.split(SENTENCE_ENDING_TOKEN))\n",
    "    sentences_by_orientation = {}\n",
    "\n",
    "# For each article, collect all sentences and sort them according to the\n",
    "# political orientation of its media outlet and the year of its publication.\n",
    "# Use a cache, if available.\n",
    "for orientation, grouping in orientation_groups.items():\n",
    "    print(\"=\" * 30, orientation)\n",
    "    articles_filtered = articles[articles.orientation.isin(grouping)]\n",
    "\n",
    "    for year in articles_filtered.year.unique():\n",
    "        print(\"=\" * 20, year)\n",
    "\n",
    "        # Collect all sentences of the current orientation and year; we don't care about document\n",
    "        # levels or specific outlets at this point, so we can simply collect them in a flat list.\n",
    "        orientation_year_articles = articles_filtered[articles_filtered.year == year]\n",
    "        print(f\"Processing {len(orientation_year_articles)} articles.\")\n",
    "        sentences_tokenized_cache_file = path.join(\n",
    "            DECON_CACHE_PATH, f\"{orientation}-{year}-sentences-tokenized.pkl\")\n",
    "\n",
    "        if path.exists(sentences_tokenized_cache_file):\n",
    "            print(\"Found tokenized cache. Loading from file.\")\n",
    "            with open(sentences_tokenized_cache_file, \"rb\") as f:\n",
    "                orientation_year_sentences_tokenized = pickle.load(f)\n",
    "        else:\n",
    "            print(\"No tokenized cache found. Tokenizing sentences now.\")\n",
    "            orientation_year_sentences_tokenized = [\n",
    "                word_tokenize(s.lower())\n",
    "                for sents in tqdm(orientation_year_articles[\"text\"]) for s in sents]\n",
    "            print(\"Caching tokenized sentences.\")\n",
    "            with open(sentences_tokenized_cache_file, \"wb\") as f:\n",
    "                pickle.dump(orientation_year_sentences_tokenized, f)\n",
    "\n",
    "        # Retrieve sentences of each token\n",
    "        print(\"Retrieving sentences.\")\n",
    "        if f\"{orientation}-{year}\" in sentences_by_orientation.keys():\n",
    "            print(\"Found orientation in cache.\")\n",
    "            orientation_year_sentences = sentences_by_orientation[f\"{orientation}-{year}\"]\n",
    "        else:\n",
    "            orientation_year_sentences = {}\n",
    "\n",
    "        # Retrieve sentences for each token and join the tokens of each sentence into a string.\n",
    "        for word in tqdm(vocab_list):\n",
    "            word_lower = word.lower()\n",
    "            if word_lower in orientation_year_sentences.keys():\n",
    "                continue\n",
    "            orientation_year_sentences[word_lower] = [\n",
    "                \" \".join(s) for s in orientation_year_sentences_tokenized if word_lower in s]\n",
    "\n",
    "        sentences_by_orientation[f\"{orientation}-{year}\"] = orientation_year_sentences\n",
    "\n",
    "# Cache final embedding dictionary\n",
    "with open(sentences_per_orientation_cache_file, \"wb\") as f:\n",
    "    pickle.dump(sentences_by_orientation, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve contextualized embeddings per token for each sentence\n",
    "def embed_token(token, sentence: str) -> list:\n",
    "    flair_sentence = Sentence(sentence)\n",
    "    embedding_model.embed(flair_sentence)\n",
    "\n",
    "    # Find token position in sentence\n",
    "    # We use the flair sentence here instead of the original string to ensure, that the index of the\n",
    "    # string and the embedding is really the same.\n",
    "    i = flair_sentence.to_original_text().split().index(token)\n",
    "\n",
    "    return flair_sentence[i].embedding\n",
    "\n",
    "# Retrieve all sentences a token appears in and generate pooled embeddings\n",
    "for orientation, grouping in orientation_groups.items():\n",
    "    print(\"=\" * 30, orientation)\n",
    "    articles_filtered = articles[articles.orientation.isin(grouping)]\n",
    "\n",
    "    for year in articles_filtered.year.unique():\n",
    "        print(\"=\" * 20, year)\n",
    "        orientation_year_context_embedded_cache_file = path.join(\n",
    "            DECON_CACHE_PATH, f\"orientation-context-embedded-{orientation}-{year}.pkl\")\n",
    "\n",
    "        if path.exists(orientation_year_context_embedded_cache_file):\n",
    "            print(\"Found existing embedding cache. Loading from file.\")\n",
    "            with open(orientation_year_context_embedded_cache_file, \"rb\") as f:\n",
    "                orientation_year_context_embeddings = pickle.load(f)\n",
    "        else:\n",
    "            print(\"No embedding cache found.\")\n",
    "            orientation_year_context_embeddings = {}\n",
    "\n",
    "        # Retrieve the contextualized embeddings per word for each orientation\n",
    "        for token, sentences in sentences_by_orientation[f\"{orientation}-{year}\"].items():\n",
    "            if len(sentences) < 1:\n",
    "                print(f\"No sentences for token '{token}'. Skipping\")\n",
    "                continue\n",
    "            elif len(sentences) > MAX_SENTENCE_NUMBER:\n",
    "                print(\n",
    "                    f\"Found {len(sentences)} sentences for '{token}'. \\\n",
    "                    Truncating to {MAX_SENTENCE_NUMBER}.\")\n",
    "                sentences = sentences[:MAX_SENTENCE_NUMBER]\n",
    "            else:\n",
    "                print(f\"Found {len(sentences)} sentences for '{token}'. No truncation necessary.\")\n",
    "\n",
    "            if token in orientation_year_context_embeddings.keys():\n",
    "                print(f\"Found '{token}' in cache. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            print(\"Generating token embeddings.\")\n",
    "            token_embeds_lst = []\n",
    "            for s in tqdm(sentences, mininterval=5, leave=False):\n",
    "                token_embeds_lst.append(embed_token(token, s))\n",
    "            token_sentence_embeddings = torch.vstack(token_embeds_lst)\n",
    "\n",
    "            orientation_year_context_embeddings[token] = {\n",
    "                \"mean_pooled_embedding\": torch.mean(token_sentence_embeddings, dim=0, keepdim=True),\n",
    "                \"max_pooled_embedding\": torch.amax(token_sentence_embeddings, dim=0, keepdim=True),\n",
    "                \"min_pooled_embedding\": torch.amin(token_sentence_embeddings, dim=0, keepdim=True)\n",
    "            }\n",
    "\n",
    "            # Cache generated embeddings to disk\n",
    "            with open(orientation_year_context_embedded_cache_file, \"wb\") as f:\n",
    "                pickle.dump(orientation_year_context_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating final token->embedding dictionary from the cached embedding file.\n",
    "# Uses the specific pooled embeddings setting defined below.\n",
    "\n",
    "vocabulary = get_test_vocabulary(word_sets=word_sets, similarity_eval_data_path=SIM_EVAL_DATA_DIR)\n",
    "vocab_list = list(vocabulary)\n",
    "pooled_embedding_type = \"mean_pooled_embedding\"\n",
    "\n",
    "# Load embedding cache\n",
    "for orientation, grouping in orientation_groups.items():\n",
    "    print(\"=\" * 30, orientation)\n",
    "    articles_filtered = articles[articles.orientation.isin(grouping)]\n",
    "\n",
    "    for year in articles_filtered.year.unique():\n",
    "        print(\"=\" * 20, year)\n",
    "\n",
    "        model_path = f\"{DECON_MODEL_PATH}/{orientation}-{year}.model\"\n",
    "        orientation_year_context_embedded_cache_file = path.join(\n",
    "            DECON_CACHE_PATH, f\"orientation-context-embedded-{orientation}-{year}.pkl\")\n",
    "\n",
    "        print(\"Loading cached orientation embeddings.\")\n",
    "        orientation_year_context_embeddings = {}\n",
    "        with open(orientation_year_context_embedded_cache_file, \"rb\") as f:\n",
    "            orientation_year_context_embeddings = pickle.load(f)\n",
    "\n",
    "        # vocab_size = len(orientation_year_context_embeddings)\n",
    "        embedding_size = len(orientation_year_context_embeddings[\"woman\"][pooled_embedding_type][0])\n",
    "\n",
    "        print(\"Copying vectors to CPU and writing to disk.\")\n",
    "        token_vector_strings = []\n",
    "        for token in tqdm(vocabulary, mininterval=2):\n",
    "            try:\n",
    "                token_vector = (\n",
    "                    orientation_year_context_embeddings[token][pooled_embedding_type][0].tolist())\n",
    "                token_vector_line = \" \".join([str(d) for d in token_vector])\n",
    "                token_vector_strings.append(f\"{token} {token_vector_line}\")\n",
    "            except KeyError:\n",
    "                print(f\"Token {token} not found. Skipping.\")\n",
    "\n",
    "        # Save final embedding dictionary to disk\n",
    "        with open(f\"{model_path}\", \"w\") as f:\n",
    "            f.write(f\"{len(token_vector_strings)} {embedding_size}\\n\")\n",
    "            for token_vector_line in token_vector_strings:\n",
    "                f.write(f\"{token_vector_line}\\n\")\n",
    "\n",
    "print(\"Generation done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "61daf9b516095512657e7198d703e6172124fdf4d3e94a95e013ddc8a0dfe156"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
